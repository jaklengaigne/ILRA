{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5853952c-2895-4d7e-91ff-16c4fe91c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02247fd-9d9c-4e39-a521-ad481e185f0d",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Your data should be downloaded from https://apps.webofknowledge.com search engine. The format should be a .csv file.\n",
    "\n",
    "Copy it in the same folder of the notebook and input the name in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09bc4f46-c833-4ddc-88ee-8e245c2e0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing files and removing duplicate from previous searches\n"
     ]
    }
   ],
   "source": [
    "current = 'ILRA_EXAMPLE_Data_rjs_cs_cc.xls'  # your new dataset\n",
    "compare = ['ILRA_EXAMPLE_Data_rotary_jet_spinning.xls']  # your previus dataset\n",
    "\n",
    "current_data = pd.read_excel(current)  # import in dataframe\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "if not compare:\n",
    "    print (\"No previous search\")\n",
    "    raw_data = current_data.copy(deep=True)\n",
    "else:\n",
    "    print (\"Comparing files and removing duplicate from previous searches\")\n",
    "    for filename in compare:\n",
    "        df_compare_1 = pd.read_excel(filename)\n",
    "        raw_data = pd.concat([df_compare_1, current_data]).drop_duplicates(keep=False).copy(deep=True)\n",
    "\n",
    "# raw_data.head  # you can uncomment this line to check that the file appears correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745b702-0d78-492a-8779-2dda67bdfa61",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "1. duplicate removal\n",
    "2. remove empty columns (NaN)\n",
    "3. text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f050ea-87c7-4006-93ec-ebb103922a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate duplicates\n",
    "raw_data['dup'] = raw_data.duplicated(subset=None, keep='first')\n",
    "# Counting the number of duplicates\n",
    "raw_data['dup'].value_counts()\n",
    "# Creating a new dataframe without the duplicates\n",
    "raw_data_noDup = raw_data[raw_data['dup'] == False]\n",
    "# Deleting the column with the True and False because because it is no more useful\n",
    "del raw_data_noDup['dup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d52f4c-2cc1-4ac2-a22d-8f0f97bb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless columns (All articles have written nothing in those fields)\n",
    "raw_data_useField = raw_data_noDup.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf1811e-b335-4d3f-abca-6c59c1572932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of NaN to string\n",
    "# (For text cleaning everything need to be string)\n",
    "raw_data_str = raw_data_useField.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "892e8d22-eaa6-4025-afbb-9e5628193679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning (removal of \"meaningless\" words)\n",
    "\"\"\"\n",
    "SmartStoplist.txt\n",
    "by Lisa Andreevna\n",
    "Lisanka93/text_analysis_python_101. (n.d.). GitHub. Retrieved May 3, 2021,\n",
    "from https://github.com/lisanka93/text_analysis_python_101\n",
    "**** Note : nan is added to Lisa Andreevna's list ****\n",
    "\"\"\"\n",
    "# Definition of constant and variable\n",
    "stop_words_file = 'SmartStoplist.txt'\n",
    "stop_words = []\n",
    "# Creating a list of stop words while reading the stop words's file\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())\n",
    "# Do not understand yet\n",
    "stop_words = stop_words\n",
    "\"\"\"\n",
    "Definition of a cleaning function (preprocess before words analysis)\n",
    "This function get a text and return a text (string) of stemmed word in\n",
    "lowercase without stop words and any caracter except letter\n",
    "\"\"\"\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Keep only letters in the text (lowercase and capitals) using Regex (re).\n",
    "    Replace all symboles with a blank space.\n",
    "    \"\"\"\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Change the capitals for lowercase AND split into a list of words (no expression)\n",
    "    words = letters_only_text.lower().split()\n",
    "    # Define a variable to receive only the useful crop (or not) words\n",
    "    cleaned_words = []\n",
    "    # Remove stop words (Take word in list of words and make a list of clean words)\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    # Stem word (Creating a new list of stemmed word with the clean one)\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = PorterStemmer().stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    # After all those changes, convert back the final list into string\n",
    "    return \" \".join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb997d44-d527-4b2d-b375-7b4d0e40a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean abstracts of all the articles of the research (overwrite)\n",
    "raw_data_str['Abstract'] = raw_data_str['Abstract'].apply(preprocess)\n",
    "raw_data_str['Article Title'] = raw_data_str['Article Title'].apply(preprocess)\n",
    "raw_data_str['Author Keywords'] = raw_data_str['Author Keywords'].apply(preprocess)\n",
    "clean_data = raw_data_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8a65-96b0-42be-aefd-dfb5c39085b9",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "1. Most common occurences\n",
    "2. Find biagrams and trigrams \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "94a1b730-4716-496b-b21c-9497691d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "\n",
    "def word_count(category, clean_data):\n",
    "    # Most common words in all the abstracts (top 100)\n",
    "    top_hundred = Counter(\" \".join(clean_data[category]).split()).most_common(100)\n",
    "\n",
    "    # Occurence of all the clean words (approximate number by trial and error)\n",
    "    clean_words_occ = Counter(\" \".join(clean_data[category]).split()).most_common(2900)\n",
    "\n",
    "    # Most common bigrams and trigrams in clean data\n",
    "    # Puting all abstracts into a list\n",
    "    all_abstracts_list = clean_data[category].tolist()\n",
    "    # Defining variables\n",
    "    all_abstracts_bigrams = []\n",
    "    all_abstracts_trigrams = []\n",
    "\n",
    "    # Creating list of bigrams and trigrams by abstracts, i.e. list[0]=allBigramOfAbs1\n",
    "    for abstracts in all_abstracts_list:\n",
    "        abstracts = word_tokenize(abstracts)\n",
    "        all_abstracts_bigrams.append(list(bigrams(abstracts)))\n",
    "        all_abstracts_trigrams.append(list(trigrams(abstracts)))\n",
    "\n",
    "    # Obtaining the most commons ones by abstracts for all of them\n",
    "    top3_bi = []\n",
    "    for bi_by_abst in all_abstracts_bigrams:\n",
    "        top3_bi_by_abst = Counter(bi_by_abst).most_common(3)\n",
    "        top3_bi.append(top3_bi_by_abst)\n",
    "    top3_tri = []\n",
    "    for tri_by_abst in all_abstracts_trigrams:\n",
    "        top3_tri_by_abst = Counter(tri_by_abst).most_common(3)\n",
    "        top3_tri.append(top3_tri_by_abst)\n",
    "\n",
    "    return top_hundred, clean_words_occ, top3_bi, top3_tri"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9ff19-775b-456e-9b64-ad7db4c69a67",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5368f51-d8e2-4bc9-8e93-43ca621f6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "category = 'Abstract'\n",
    "\n",
    "top_hundred, clean_word_occ, top3_bi, top3_tri = word_count(category, clean_data)\n",
    "# uncomment to get a look\n",
    "#top_hundred\n",
    "#(top3_bi)\n",
    "#(top3_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4218d-daea-4288-a81d-3ceca93029f3",
   "metadata": {},
   "source": [
    "## Analysis in the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3c18b8c-31d9-4ad3-bb1c-93bf2fcf5db7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rotari', 29),\n",
       " ('jet', 29),\n",
       " ('spin', 26),\n",
       " ('scaffold', 15),\n",
       " ('nanofib', 14),\n",
       " ('fiber', 12),\n",
       " ('tissu', 9),\n",
       " ('polym', 7),\n",
       " ('produc', 7),\n",
       " ('spun', 6),\n",
       " ('product', 6),\n",
       " ('electrospin', 6),\n",
       " ('engin', 5),\n",
       " ('high', 5),\n",
       " ('fibrou', 5),\n",
       " ('applic', 5),\n",
       " ('poli', 4),\n",
       " ('polycaprolacton', 4),\n",
       " ('fabric', 4),\n",
       " ('nanofibr', 4),\n",
       " ('centrifug', 3),\n",
       " ('obtain', 3),\n",
       " ('potenti', 3),\n",
       " ('biomimet', 3),\n",
       " ('repair', 3),\n",
       " ('low', 3),\n",
       " ('hybrid', 3),\n",
       " ('model', 3),\n",
       " ('porou', 3),\n",
       " ('protein', 3),\n",
       " ('multi', 3),\n",
       " ('nanotub', 3),\n",
       " ('heart', 3),\n",
       " ('oxid', 2),\n",
       " ('beta', 2),\n",
       " ('vitro', 2),\n",
       " ('vivo', 2),\n",
       " ('evalu', 2),\n",
       " ('effect', 2),\n",
       " ('composit', 2),\n",
       " ('solvent', 2),\n",
       " ('techniqu', 2),\n",
       " ('base', 2),\n",
       " ('scale', 2),\n",
       " ('deliveri', 2),\n",
       " ('estrogen', 2),\n",
       " ('promot', 2),\n",
       " ('skin', 2),\n",
       " ('nano', 2),\n",
       " ('hydroxyapatit', 2),\n",
       " ('influenc', 2),\n",
       " ('polyurethan', 2),\n",
       " ('gelatin', 2),\n",
       " ('comparison', 2),\n",
       " ('pcl', 2),\n",
       " ('acid', 2),\n",
       " ('extracellular', 2),\n",
       " ('matrix', 2),\n",
       " ('membran', 2),\n",
       " ('plla', 2),\n",
       " ('system', 2),\n",
       " ('micro', 2),\n",
       " ('wall', 2),\n",
       " ('carbon', 2),\n",
       " ('airbrush', 2),\n",
       " ('orthoped', 2),\n",
       " ('regener', 2),\n",
       " ('wound', 2),\n",
       " ('manufactur', 2),\n",
       " ('nanocomposit', 2),\n",
       " ('cl', 2),\n",
       " ('apatit', 2),\n",
       " ('fibr', 2),\n",
       " ('morpholog', 2),\n",
       " ('simpl', 2),\n",
       " ('ethylen', 1),\n",
       " ('rival', 1),\n",
       " ('properti', 1),\n",
       " ('electrospun', 1),\n",
       " ('tricalcium', 1),\n",
       " ('phosphat', 1),\n",
       " ('synergist', 1),\n",
       " ('volatil', 1),\n",
       " ('collector', 1),\n",
       " ('distanc', 1),\n",
       " ('pullulan', 1),\n",
       " ('pva', 1),\n",
       " ('tubular', 1),\n",
       " ('collagen', 1),\n",
       " ('elastin', 1),\n",
       " ('dual', 1),\n",
       " ('phase', 1),\n",
       " ('tendon', 1),\n",
       " ('develop', 1),\n",
       " ('laboratori', 1),\n",
       " ('speed', 1),\n",
       " ('devic', 1),\n",
       " ('pharmaceut', 1),\n",
       " ('microfibr', 1),\n",
       " ('drug', 1)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'Article Title'\n",
    "\n",
    "#top_hundred, clean_word_occ, top3_bi, top3_tri = word_count(category, clean_data)\n",
    "# uncomment to get a look\n",
    "#top_hundred\n",
    "#(top3_bi)\n",
    "#(top3_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ed7d4-dc10-4fb2-bcef-bd08a2bf4446",
   "metadata": {},
   "source": [
    "## Analysis in the Author's keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d96a587b-5ca5-4867-9586-e4da68c17c9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('spinning;', 27),\n",
       " ('jet', 18),\n",
       " ('Rotary', 13),\n",
       " ('rotary', 10),\n",
       " ('NaN', 10),\n",
       " ('centrifugal', 5),\n",
       " ('tissue', 5),\n",
       " ('engineering', 5),\n",
       " ('Tissue', 5),\n",
       " ('engineering;', 5),\n",
       " ('spinning', 4),\n",
       " ('Nanofiber;', 4),\n",
       " ('Electrospinning;', 4),\n",
       " ('nanofibers;', 4),\n",
       " ('electrospinning;', 3),\n",
       " ('healing;', 3),\n",
       " ('fibers;', 3),\n",
       " ('scaffold;', 3),\n",
       " ('Centrifugal', 3),\n",
       " ('carbon', 3),\n",
       " ('Nanofiber', 3),\n",
       " ('fiber', 2),\n",
       " ('rheology;', 2),\n",
       " ('materials;', 2),\n",
       " ('Bone', 2),\n",
       " ('jet-spinning;', 2),\n",
       " ('Fibers;', 2),\n",
       " ('of', 2),\n",
       " ('biomaterials;', 2),\n",
       " ('fiber;', 2),\n",
       " ('polyurethane;', 2),\n",
       " ('scaffolds', 2),\n",
       " ('mechanical', 2),\n",
       " ('Wound', 2),\n",
       " ('immersion', 2),\n",
       " ('nanofiber;', 2),\n",
       " ('composites;', 2),\n",
       " ('Rapid', 2),\n",
       " ('acid);', 2),\n",
       " ('Biomaterial;', 2),\n",
       " ('Polymer', 2),\n",
       " ('nanotubes;', 2),\n",
       " ('Spinning;', 2),\n",
       " ('applications;', 2),\n",
       " ('valve;', 2),\n",
       " ('Jet', 2),\n",
       " ('entangled', 1),\n",
       " ('polymers;', 1),\n",
       " ('nonwovens;', 1),\n",
       " ('Biocompatible', 1),\n",
       " ('transplantation;', 1),\n",
       " ('Regeneration', 1),\n",
       " ('Pullulan;', 1),\n",
       " ('Design', 1),\n",
       " ('experiments;', 1),\n",
       " ('Scaffold', 1),\n",
       " ('membranes;', 1),\n",
       " ('proteins;', 1),\n",
       " ('tubular', 1),\n",
       " ('PCL;', 1),\n",
       " ('gelatin;', 1),\n",
       " ('growth', 1),\n",
       " ('differentiation', 1),\n",
       " ('factor', 1),\n",
       " ('5;', 1),\n",
       " ('wet', 1),\n",
       " ('tendon', 1),\n",
       " ('High-speed', 1),\n",
       " ('Finite-element', 1),\n",
       " ('modelling', 1),\n",
       " ('the', 1),\n",
       " ('load;', 1),\n",
       " ('Platform', 1),\n",
       " ('technology;', 1),\n",
       " ('Microfibres;', 1),\n",
       " ('Positron', 1),\n",
       " ('annihilation', 1),\n",
       " ('lifetime', 1),\n",
       " ('spectroscopy', 1),\n",
       " ('(PALS);', 1),\n",
       " ('Morphological', 1),\n",
       " ('analysis', 1),\n",
       " ('Soy', 1),\n",
       " ('phytoestrogen;', 1),\n",
       " ('Immersion', 1),\n",
       " ('Estrogen', 1),\n",
       " ('receptor', 1),\n",
       " ('beta', 1),\n",
       " ('Implant;', 1),\n",
       " ('Membrane;', 1),\n",
       " ('Polymer;', 1),\n",
       " ('Protein;', 1),\n",
       " ('Poly', 1),\n",
       " ('epsilon-caprolactone;', 1),\n",
       " ('Bacteria', 1),\n",
       " ('colonization;', 1),\n",
       " ('Cytotoxicity;', 1),\n",
       " ('Nanotechnology', 1),\n",
       " ('theory', 1),\n",
       " ('and', 1)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category = 'Author Keywords'\n",
    "\n",
    "#top_hundred, clean_word_occ, top3_bi, top3_tri = word_count(category, clean_data)\n",
    "# uncomment to get a look\n",
    "#top_hundred\n",
    "#(top3_bi)\n",
    "#(top3_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc29012-cb0b-40a9-9ec4-a68860be1eff",
   "metadata": {},
   "source": [
    "# Analysis in the Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c43404-2a42-41a1-8985-68717f85e0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1afbe41-e14f-4b1d-a8d6-0bcfe300eebe",
   "metadata": {},
   "source": [
    "# Data save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d30c36-b40d-4064-b2f3-4bbe4a222403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no folder for the result create one\n",
    "os.makedirs('Results', exist_ok=True)\n",
    "\n",
    "# Word count data\n",
    "clean_words_occ_df = pd.DataFrame(clean_words_occ, columns=['Word', 'Count'])\n",
    "clean_words_occ_df.to_csv('./Results/ILRA_CleanWordsOccurence.csv', sep=';')\n",
    "\n",
    "# Bigrams and trigrams TO CORRECT\n",
    "abstract_grams_df = pd.DataFrame([all_abstracts_bigrams, all_abstracts_trigrams])\n",
    "abstract_grams_df.to_csv('./Results/ILRA_abstract_grams.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "99e2d56e-740e-44be-835a-d0e09f5f5632",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Publication Type', 'Authors', 'Book Editors', 'Book Group Authors',\n",
       "       'Author Full Names', 'Article Title', 'Source Title',\n",
       "       'Book Series Title', 'Language', 'Document Type', 'Conference Title',\n",
       "       'Conference Date', 'Conference Location', 'Conference Sponsor',\n",
       "       'Author Keywords', 'Keywords Plus', 'Abstract', 'Addresses',\n",
       "       'Reprint Addresses', 'Email Addresses', 'Researcher Ids', 'ORCIDs',\n",
       "       'Funding Orgs', 'Funding Text', 'Cited Reference Count',\n",
       "       'Times Cited, WoS Core', 'Times Cited, All Databases',\n",
       "       '180 Day Usage Count', 'Since 2013 Usage Count', 'Publisher',\n",
       "       'Publisher City', 'Publisher Address', 'ISSN', 'eISSN', 'ISBN',\n",
       "       'Journal Abbreviation', 'Journal ISO Abbreviation', 'Publication Date',\n",
       "       'Publication Year', 'Volume', 'Issue', 'Supplement', 'Meeting Abstract',\n",
       "       'Start Page', 'End Page', 'Article Number', 'DOI', 'Early Access Date',\n",
       "       'Number of Pages', 'WoS Categories', 'Research Areas', 'IDS Number',\n",
       "       'UT (Unique WOS ID)', 'Pubmed Id', 'Open Access Designations',\n",
       "       'Date of Export'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6664f6-a432-4d00-b93c-903bafd2a010",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
