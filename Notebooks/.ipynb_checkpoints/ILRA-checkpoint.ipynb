{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5853952c-2895-4d7e-91ff-16c4fe91c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02247fd-9d9c-4e39-a521-ad481e185f0d",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Your data should be downloaded from https://apps.webofknowledge.com search engine. The format should be a .csv file.\n",
    "\n",
    "Copy it in the same folder of the notebook and input the name in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "09bc4f46-c833-4ddc-88ee-8e245c2e0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing files and removing duplicate from previous searches\n"
     ]
    }
   ],
   "source": [
    "current = 'ILRA_EXAMPLE_Data_rjs_cs_cc.xls'  # your new dataset\n",
    "compare = ['ILRA_EXAMPLE_Data_rotary_jet_spinning.xls']  # your previus dataset\n",
    "\n",
    "current_data = pd.read_excel(current)  # import in dataframe\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "if not compare:\n",
    "    print (\"No previous search\")\n",
    "    raw_data = current_data.copy(deep=True)\n",
    "else:\n",
    "    print (\"Comparing files and removing duplicate from previous searches\")\n",
    "    for filename in compare:\n",
    "        df_compare_1 = pd.read_excel(filename)\n",
    "        raw_data = pd.concat([df_compare_1, current_data]).drop_duplicates(keep=False).copy(deep=True)\n",
    "\n",
    "# raw_data.head  # you can uncomment this line to check that the file appears correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745b702-0d78-492a-8779-2dda67bdfa61",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "1. duplicate removal\n",
    "2. remove empty columns (NaN)\n",
    "3. text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f050ea-87c7-4006-93ec-ebb103922a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate duplicates\n",
    "raw_data['dup'] = raw_data.duplicated(subset=None, keep='first')\n",
    "# Counting the number of duplicates\n",
    "raw_data['dup'].value_counts()\n",
    "# Creating a new dataframe without the duplicates\n",
    "raw_data_noDup = raw_data[raw_data['dup'] == False]\n",
    "# Deleting the column with the True and False because because it is no more useful\n",
    "del raw_data_noDup['dup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d52f4c-2cc1-4ac2-a22d-8f0f97bb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless columns (All articles have written nothing in those fields)\n",
    "raw_data_useField = raw_data_noDup.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf1811e-b335-4d3f-abca-6c59c1572932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of NaN to string\n",
    "# (For text cleaning everything need to be string)\n",
    "raw_data_str = raw_data_useField.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892e8d22-eaa6-4025-afbb-9e5628193679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning (removal of \"meaningless\" words)\n",
    "\"\"\"\n",
    "SmartStoplist.txt\n",
    "by Lisa Andreevna\n",
    "Lisanka93/text_analysis_python_101. (n.d.). GitHub. Retrieved May 3, 2021,\n",
    "from https://github.com/lisanka93/text_analysis_python_101\n",
    "**** Note : nan is added to Lisa Andreevna's list ****\n",
    "\"\"\"\n",
    "# Definition of constant and variable\n",
    "stop_words_file = 'SmartStoplist.txt'\n",
    "stop_words = []\n",
    "# Creating a list of stop words while reading the stop words's file\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())\n",
    "# Do not understand yet\n",
    "stop_words = stop_words\n",
    "\"\"\"\n",
    "Definition of a cleaning function (preprocess before words analysis)\n",
    "This function get a text and return a text (string) of stemmed word in\n",
    "lowercase without stop words and any caracter except letter\n",
    "\"\"\"\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Keep only letters in the text (lowercase and capitals) using Regex (re).\n",
    "    Replace all symboles with a blank space.\n",
    "    \"\"\"\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Change the capitals for lowercase AND split into a list of words (no expression)\n",
    "    words = letters_only_text.lower().split()\n",
    "    # Define a variable to receive only the useful crop (or not) words\n",
    "    cleaned_words = []\n",
    "    # Remove stop words (Take word in list of words and make a list of clean words)\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    # Stem word (Creating a new list of stemmed word with the clean one)\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = PorterStemmer().stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    # After all those changes, convert back the final list into string\n",
    "    return \" \".join(stemmed_words)\n",
    "# Clean abstracts of all the articles of the research (overwrite)\n",
    "raw_data_str['Abstract'] = raw_data_str['Abstract'].apply(preprocess)\n",
    "clean_data = raw_data_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8a65-96b0-42be-aefd-dfb5c39085b9",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "1. Most common occurences\n",
    "2. Find biagrams and trigrams \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9ff19-775b-456e-9b64-ad7db4c69a67",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a1b730-4716-496b-b21c-9497691d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "# Most common words in all the abstracts (top 100)\n",
    "top_hundred = Counter(\" \".join(clean_data['Abstract']).split()).most_common(100)\n",
    "\n",
    "# Occurence of all the clean words (approximate number by trial and error)\n",
    "clean_words_occ = Counter(\" \".join(clean_data['Abstract']).split()).most_common(2900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ce232b-be53-419a-b876-5d032cfb81fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('fiber', 111), ('spin', 86), ('scaffold', 84), ('jet', 74), ('polym', 71), ('nanofib', 59), ('rotari', 58), ('tissu', 53), ('cell', 52), ('product', 45), ('rj', 42), ('produc', 38), ('properti', 38), ('materi', 35), ('pcl', 34), ('fabric', 34), ('high', 33), ('engin', 32), ('method', 32), ('mechan', 30), ('solut', 29), ('process', 29), ('show', 27), ('diamet', 26), ('studi', 25), ('centrifug', 24), ('electrospin', 24), ('format', 24), ('techniqu', 24), ('applic', 24), ('morpholog', 23), ('solvent', 22), ('fibr', 22), ('obtain', 21), ('fibrou', 20), ('nhap', 20), ('demonstr', 20), ('structur', 20), ('base', 19), ('composit', 18), ('align', 18), ('ecm', 18), ('model', 17), ('result', 17), ('manufactur', 17), ('poli', 16), ('compar', 16), ('design', 16), ('similar', 16), ('bone', 15), ('potenti', 15), ('heal', 15), ('gelatin', 15), ('protein', 15), ('spun', 14), ('evalu', 14), ('membran', 14), ('control', 14), ('paramet', 14), ('combin', 14), ('speed', 14), ('test', 14), ('present', 13), ('polymer', 13), ('reduc', 13), ('system', 13), ('orient', 13), ('develop', 13), ('nanofibr', 13), ('rate', 12), ('vitro', 12), ('beta', 12), ('increas', 12), ('surfac', 12), ('mat', 12), ('analysi', 12), ('effect', 12), ('form', 12), ('approach', 12), ('forc', 11), ('thermal', 11), ('concentr', 11), ('viscos', 11), ('repair', 11), ('promis', 11), ('rotat', 11), ('function', 11), ('synthet', 11), ('microscopi', 11), ('wound', 11), ('addit', 11), ('cytotox', 11), ('report', 10), ('vivo', 10), ('perform', 10), ('decreas', 10), ('requir', 10), ('biocompat', 10), ('natur', 10), ('larg', 10)]\n"
     ]
    }
   ],
   "source": [
    "print(top_hundred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "303c03df-c20d-4a5a-b031-64bff11e3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common bigrams and trigrams in clean data\n",
    "# Puting all abstracts into a list\n",
    "all_abstracts_list = clean_data['Abstract'].tolist()\n",
    "# Defining variables\n",
    "all_abstracts_bigrams = []\n",
    "all_abstracts_trigrams = []\n",
    "\n",
    "# Creating list of bigrams and trigrams by abstracts, i.e. list[0]=allBigramOfAbs1\n",
    "for abstracts in all_abstracts_list:\n",
    "    abstracts = word_tokenize(abstracts)\n",
    "    all_abstracts_bigrams.append(list(bigrams(abstracts)))\n",
    "    all_abstracts_trigrams.append(list(trigrams(abstracts)))\n",
    "\n",
    "# Obtaining the most commons ones by abstracts for all of them\n",
    "top3_bi = []\n",
    "for bi_by_abst in all_abstracts_bigrams:\n",
    "    top3_bi_by_abst = Counter(bi_by_abst).most_common(3)\n",
    "    top3_bi.append(top3_bi_by_abst)\n",
    "top3_tri = []\n",
    "for tri_by_abst in all_abstracts_trigrams:\n",
    "    top3_tri_by_abst = Counter(tri_by_abst).most_common(3)\n",
    "    top3_tri.append(top3_tri_by_abst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d5368f51-d8e2-4bc9-8e93-43ca621f6daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to get a look\n",
    "#(top3_bi)\n",
    "#(top3_tri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4218d-daea-4288-a81d-3ceca93029f3",
   "metadata": {},
   "source": [
    "## Analysis in the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c18b8c-31d9-4ad3-bb1c-93bf2fcf5db7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "549ed7d4-dc10-4fb2-bcef-bd08a2bf4446",
   "metadata": {},
   "source": [
    "## Analysis in the Author's keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a587b-5ca5-4867-9586-e4da68c17c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecc29012-cb0b-40a9-9ec4-a68860be1eff",
   "metadata": {},
   "source": [
    "# Analysis in the Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c43404-2a42-41a1-8985-68717f85e0ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e1afbe41-e14f-4b1d-a8d6-0bcfe300eebe",
   "metadata": {},
   "source": [
    "# Data save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d30c36-b40d-4064-b2f3-4bbe4a222403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no folder for the result create one\n",
    "os.makedirs('Results', exist_ok=True)\n",
    "\n",
    "# Word count data\n",
    "clean_words_occ_df = pd.DataFrame(clean_words_occ, columns=['Word', 'Count'])\n",
    "clean_words_occ_df.to_csv('./Results/ILRA_CleanWordsOccurence.csv', sep=';')\n",
    "\n",
    "# Bigrams and trigrams TO CORRECT\n",
    "abstract_grams_df = pd.DataFrame([all_abstracts_bigrams, all_abstracts_trigrams])\n",
    "abstract_grams_df.to_csv('./Results/ILRA_abstract_grams.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9634b-6666-4cf7-b8a3-dbc8f5d428cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
