{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5853952c-2895-4d7e-91ff-16c4fe91c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import word_tokenize, bigrams, trigrams\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02247fd-9d9c-4e39-a521-ad481e185f0d",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Your data should be downloaded from https://apps.webofknowledge.com search engine. The format should be a .csv file.\n",
    "\n",
    "Copy it in the same folder of the notebook and input the name in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09bc4f46-c833-4ddc-88ee-8e245c2e0243",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"Data_rotary_jet_spinning\"\n",
    "\n",
    "raw_data = pd.read_excel(filename+\".xls\")\n",
    "\n",
    "# raw_data.head  # you can uncomment this line to check that the file appears correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745b702-0d78-492a-8779-2dda67bdfa61",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "1. duplicate removal\n",
    "2. remove empty columns (NaN)\n",
    "3. text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f050ea-87c7-4006-93ec-ebb103922a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate duplicates\n",
    "raw_data['dup'] = raw_data.duplicated(subset=None, keep='first')\n",
    "# Counting the number of duplicates\n",
    "raw_data['dup'].value_counts()\n",
    "# Creating a new dataframe without the duplicates\n",
    "raw_data_noDup = raw_data[raw_data['dup'] == False]\n",
    "# Deleting the column with the True and False because because it is no more useful\n",
    "del raw_data_noDup['dup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55d52f4c-2cc1-4ac2-a22d-8f0f97bb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless columns (All articles have written nothing in those fields)\n",
    "raw_data_useField = raw_data_noDup.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0bf1811e-b335-4d3f-abca-6c59c1572932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of NaN to string\n",
    "# (For text cleaning everything need to be string)\n",
    "raw_data_str = raw_data_useField.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892e8d22-eaa6-4025-afbb-9e5628193679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning (removal of \"meaningless\" words)\n",
    "\"\"\"\n",
    "SmartStoplist.txt\n",
    "by Lisa Andreevna\n",
    "Lisanka93/text_analysis_python_101. (n.d.). GitHub. Retrieved May 3, 2021,\n",
    "from https://github.com/lisanka93/text_analysis_python_101\n",
    "**** Note : nan is added to Lisa Andreevna's list ****\n",
    "\"\"\"\n",
    "# Definition of constant and variable\n",
    "stop_words_file = 'SmartStoplist.txt'\n",
    "stop_words = []\n",
    "# Creating a list of stop words while reading the stop words's file\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())\n",
    "# Do not understand yet\n",
    "stop_words = stop_words\n",
    "\"\"\"\n",
    "Definition of a cleaning function (preprocess before words analysis)\n",
    "This function get a text and return a text (string) of stemmed word in\n",
    "lowercase without stop words and any caracter except letter\n",
    "\"\"\"\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Keep only letters in the text (lowercase and capitals) using Regex (re).\n",
    "    Replace all symboles with a blank space.\n",
    "    \"\"\"\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Change the capitals for lowercase AND split into a list of words (no expression)\n",
    "    words = letters_only_text.lower().split()\n",
    "    # Define a variable to receive only the useful crop (or not) words\n",
    "    cleaned_words = []\n",
    "    # Remove stop words (Take word in list of words and make a list of clean words)\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    # Stem word (Creating a new list of stemmed word with the clean one)\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = PorterStemmer().stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    # After all those changes, convert back the final list into string\n",
    "    return \" \".join(stemmed_words)\n",
    "# Clean abstracts of all the articles of the research (overwrite)\n",
    "raw_data_str['Abstract'] = raw_data_str['Abstract'].apply(preprocess)\n",
    "clean_data = raw_data_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8a65-96b0-42be-aefd-dfb5c39085b9",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "1. Most common occurences\n",
    "2. Find biagrams and trigrams \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94a1b730-4716-496b-b21c-9497691d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "# Most common words in all the abstracts (top 100)\n",
    "top_hundred = Counter(\" \".join(clean_data['Abstract']).split()).most_common(100)\n",
    "\n",
    "# Occurence of all the clean words (approximate number by trial and error)\n",
    "clean_words_occ = Counter(\" \".join(clean_data['Abstract']).split()).most_common(2900)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "303c03df-c20d-4a5a-b031-64bff11e3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common bigrams and trigrams in clean data\n",
    "# Puting all abstracts into a list\n",
    "all_abstracts_list = clean_data['Abstract'].tolist()\n",
    "# Defining variables\n",
    "all_abstracts_bigrams = []\n",
    "all_abstracts_trigrams = []\n",
    "\n",
    "# Creating list of bigrams and trigrams by abstracts, i.e. list[0]=allBigramOfAbs1\n",
    "for abstracts in all_abstracts_list:\n",
    "    abstracts = word_tokenize(abstracts)\n",
    "    all_abstracts_bigrams.append(list(bigrams(abstracts)))\n",
    "    all_abstracts_trigrams.append(list(trigrams(abstracts)))\n",
    "\n",
    "# Obtaining the most commons ones by abstracts for all of them\n",
    "top3_bi = []\n",
    "for bi_by_abst in all_abstracts_bigrams:\n",
    "    top3_bi_by_abst = Counter(bi_by_abst).most_common(3)\n",
    "    top3_bi.append(top3_bi_by_abst)\n",
    "top3_tri = []\n",
    "for tri_by_abst in all_abstracts_trigrams:\n",
    "    top3_tri_by_abst = Counter(tri_by_abst).most_common(3)\n",
    "    top3_tri.append(top3_tri_by_abst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afbe41-e14f-4b1d-a8d6-0bcfe300eebe",
   "metadata": {},
   "source": [
    "# Data save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3d30c36-b40d-4064-b2f3-4bbe4a222403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If there is no folder for the result create one\n",
    "os.makedirs('Results', exist_ok=True)\n",
    "\n",
    "# Word count data\n",
    "clean_words_occ_df = pd.DataFrame(clean_words_occ, columns=['Word', 'Count'])\n",
    "clean_words_occ_df.to_csv('./Results/CleanWordsOccurence.csv', sep=';')\n",
    "\n",
    "# Bigrams and trigrams TO CORRECT\n",
    "abstract_grams_df = pd.DataFrame([all_abstracts_bigrams, all_abstracts_trigrams])\n",
    "abstract_grams_df.to_csv('./Results/abstract_grams.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a9634b-6666-4cf7-b8a3-dbc8f5d428cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
