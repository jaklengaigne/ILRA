{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5853952c-2895-4d7e-91ff-16c4fe91c7e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Majikthise\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Librairies\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02247fd-9d9c-4e39-a521-ad481e185f0d",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "Your data should be downloaded from https://apps.webofknowledge.com search engine. The format should be a .xls file.\n",
    "\n",
    "Copy it in the same folder of the notebook and input the name in the code below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93ffe54b-7ca1-458f-ab8b-36abb2b97a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS THE ONLY CELL YOU SHOULD HAVE TO EDIT. For now.\n",
    "\n",
    "username = 'your_username'\n",
    "trialnumber = '000'  # let be safe use 3 numbers starting at 000.\n",
    "exportnumber = '000'  # let be safe use 3 numbers starting at 000.\n",
    "\n",
    "\n",
    "current = './data_examples/ILRA_EXAMPLE_Data_rjs_cs_cc.xls'  # your new dataset\n",
    "compare_data = ['./data_examples/ILRA_EXAMPLE_Data_rotary_jet_spinning.xls']  # your previus datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09bc4f46-c833-4ddc-88ee-8e245c2e0243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No previous search\n",
      "275\n",
      "Exporting ID for files: your_username_20210928_000_000\n"
     ]
    }
   ],
   "source": [
    "# Ignore: testing data for the duplicates and select new entries algorithms.\n",
    "#current = './data_examples/ILRA_EXAMPLE_Data_rotary_jet_spinning_short.xls'  # your new dataset\n",
    "#compare_data = ['./data_examples/ILRA_EXAMPLE_Data_rotary_jet_spinning_end.xls', './data_examples/ILRA_EXAMPLE_Data_rotary_jet_spinning_long.xls']  # your previus datasets\n",
    "\n",
    "exportdate = date.today().strftime(\"%Y%m%d\")  # get the date, ISO format\n",
    "file_export_ID = username+'_'+exportdate+'_'+exportnumber+'_'+trialnumber\n",
    "\n",
    "current_data = pd.read_excel(current)  # import current xls in dataframe\n",
    "\n",
    "compare_list = []\n",
    "raw_data = pd.DataFrame()\n",
    "\n",
    "if not compare_data:\n",
    "    print(\"No previous search\")\n",
    "    raw_data = current_data.copy(deep=True)\n",
    "    print(len(raw_data))\n",
    "else:\n",
    "    print (\"Comparing files and removing duplicate from previous searches\")\n",
    "    for filename in compare_data:\n",
    "        compare_list.append(pd.read_excel(filename))\n",
    "        \n",
    "    # recursively remove previous data and keep only new entries.\n",
    "    compare_df = pd.concat(compare_list).drop_duplicates(keep='first').copy(deep=True)  # compare all and remove duplicates (keeping first apparition)\n",
    "    raw_data = pd.concat([current_data, compare_df]).drop_duplicates(keep='first').copy(deep=True)  # compare with current data to keep all differences\n",
    "    raw_data = pd.concat([compare_df, raw_data]).drop_duplicates(keep=False).copy(deep=True)  # compare with previous data to keep only the new data (remove all duplicates)\n",
    "    print('Number of new entries:', len(raw_data))\n",
    "\n",
    "\n",
    "print('Exporting ID for files:', file_export_ID)\n",
    "# raw_data.head  # you can uncomment this line to check that the file appears correctly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b745b702-0d78-492a-8779-2dda67bdfa61",
   "metadata": {},
   "source": [
    "# Data cleaning\n",
    "\n",
    "1. duplicate removal\n",
    "2. remove empty columns (NaN)\n",
    "3. text cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f050ea-87c7-4006-93ec-ebb103922a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# locate duplicates\n",
    "raw_data['dup'] = raw_data.duplicated(subset=None, keep='first')\n",
    "# Counting the number of duplicates\n",
    "raw_data['dup'].value_counts()\n",
    "# Creating a new dataframe without the duplicates\n",
    "raw_data_noDup = raw_data[raw_data['dup'] == False]\n",
    "# Deleting the column with the True and False because because it is no more useful\n",
    "del raw_data_noDup['dup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d52f4c-2cc1-4ac2-a22d-8f0f97bb8255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing useless columns (All articles have written nothing in those fields)\n",
    "raw_data_useField = raw_data_noDup.dropna(axis=1, how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf1811e-b335-4d3f-abca-6c59c1572932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing the type of NaN to string\n",
    "# (For text cleaning everything need to be string)\n",
    "raw_data_str = raw_data_useField.fillna(\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892e8d22-eaa6-4025-afbb-9e5628193679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning (removal of \"meaningless\" words)\n",
    "\"\"\"\n",
    "SmartStoplist.txt\n",
    "by Lisa Andreevna\n",
    "Lisanka93/text_analysis_python_101. (n.d.). GitHub. Retrieved May 3, 2021,\n",
    "from https://github.com/lisanka93/text_analysis_python_101\n",
    "**** Note : nan is added to Lisa Andreevna's list ****\n",
    "\"\"\"\n",
    "# Definition of constant and variable\n",
    "stop_words_file = 'SmartStoplist.txt'\n",
    "stop_words = []\n",
    "# Creating a list of stop words while reading the stop words's file\n",
    "with open(stop_words_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        stop_words.extend(line.split())\n",
    "# Do not understand yet\n",
    "stop_words = stop_words\n",
    "\"\"\"\n",
    "Definition of a cleaning function (preprocess before words analysis)\n",
    "This function get a text and return a text (string) of stemmed word in\n",
    "lowercase without stop words and any caracter except letter\n",
    "\"\"\"\n",
    "def preprocess(raw_text):\n",
    "    \"\"\"\n",
    "    Keep only letters in the text (lowercase and capitals) using Regex (re).\n",
    "    Replace all symboles with a blank space.\n",
    "    \"\"\"\n",
    "    letters_only_text = re.sub(\"[^a-zA-Z]\", \" \", raw_text)\n",
    "    # Change the capitals for lowercase AND split into a list of words (no expression)\n",
    "    words = letters_only_text.lower().split()\n",
    "    # Define a variable to receive only the useful crop (or not) words\n",
    "    cleaned_words = []\n",
    "    # Remove stop words (Take word in list of words and make a list of clean words)\n",
    "    for word in words:\n",
    "        if word not in stop_words:\n",
    "            cleaned_words.append(word)\n",
    "    # Stem word (Creating a new list of stemmed word with the clean one)\n",
    "    stemmed_words = []\n",
    "    for word in cleaned_words:\n",
    "        word = nltk.stem.PorterStemmer().stem(word)\n",
    "        stemmed_words.append(word)\n",
    "    # After all those changes, convert back the final list into string\n",
    "    return \" \".join(stemmed_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb997d44-d527-4b2d-b375-7b4d0e40a1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean abstracts of all the articles of the research (overwrite)\n",
    "raw_data_str['Abstract'] = raw_data_str['Abstract'].apply(preprocess)\n",
    "raw_data_str['Article Title'] = raw_data_str['Article Title'].apply(preprocess)\n",
    "raw_data_str['Author Keywords'] = raw_data_str['Author Keywords'].apply(preprocess)\n",
    "clean_data = raw_data_str.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4cd8a65-96b0-42be-aefd-dfb5c39085b9",
   "metadata": {},
   "source": [
    "# Data exploration\n",
    "\n",
    "The following cell is function which will extract the number of occurences and the series of bigrams and trigrams in the data.\n",
    "\n",
    "The defined function is then applied to the data, and return the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a1b730-4716-496b-b21c-9497691d2c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count\n",
    "\n",
    "def word_count(data):\n",
    "    # input: data is 1 dataframe column containing the text for each entry in the dataset\n",
    "    # Occurence of all the words\n",
    "    # convert to a dataframe\n",
    "    words_occ = pd.DataFrame(Counter(\" \".join(data).split()).most_common(), columns=['word', 'count'])\n",
    "\n",
    "    # Most common bigrams and trigrams in clean data\n",
    "    # Puting all abstracts into a list\n",
    "    data_list = data.tolist()\n",
    "    # Defining variables\n",
    "    all_bigrams = []\n",
    "    all_trigrams = []\n",
    "\n",
    "    # Creating list of bigrams and trigrams by abstracts, i.e. list[0]=allBigramOfAbs1\n",
    "    for item in data_list:\n",
    "        item_token = nltk.word_tokenize(item)\n",
    "        all_bigrams.append(list(nltk.bigrams(item_token)))\n",
    "        all_trigrams.append(list(nltk.trigrams(item_token)))\n",
    "\n",
    "    # count occurences and ranking grams\n",
    "    all_bigrams = list(itertools.chain.from_iterable(all_bigrams))  # reduce list of list to one list\n",
    "    bigrams_occ = Counter(all_bigrams).most_common()\n",
    "    \n",
    "    all_trigrams = list(itertools.chain.from_iterable(all_trigrams))  # reduce list of list to one list\n",
    "    trigrams_occ = Counter(all_trigrams).most_common()\n",
    "    \n",
    "    # Obtaining the most commons ones by abstracts for all of them\n",
    "    '''\n",
    "    top3_bi = []\n",
    "    for bi_by_abst in all_bigrams:\n",
    "        top3_bi_by_abst = Counter(bi_by_abst).most_common(3)\n",
    "        top3_bi.append(top3_bi_by_abst)\n",
    "    top3_tri = []\n",
    "    for tri_by_abst in all_trigrams:\n",
    "        top3_tri_by_abst = Counter(tri_by_abst).most_common(3)\n",
    "        top3_tri.append(top3_tri_by_abst)\n",
    "    #print(top3_tri)\n",
    "    #abstract_grams_df = pd.DataFrame([list_bigrams, list_trigrams])\n",
    "    '''\n",
    "    return words_occ, bigrams_occ, trigrams_occ\n",
    "\n",
    "#rval_abstracts = word_count(clean_data['Abstract'])\n",
    "#rval_abstracts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85214e3b-3040-4eed-9ca9-7277112ceb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(Counter(list(itertools.chain.from_iterable(rval_abstracts[1][0:1]))).most_common())\n",
    "#a = (Counter(list(itertools.chain.from_iterable(rval_abstracts[1])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc29012-cb0b-40a9-9ec4-a68860be1eff",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "This function will plot a graph of the top 10 words in the data category we analysed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c43404-2a42-41a1-8985-68717f85e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def barchart_top10(rval, plottitle):\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    ax.barh(rval[0]['word'][0:50], rval[0]['count'][0:50])\n",
    "    ax.invert_yaxis()  # labels read top-to-bottom\n",
    "    ax.set_xlabel('Count')\n",
    "    ax.set_title('Word count - top 10 in ' + plottitle)\n",
    "    \n",
    "    ### SAVE A BIG GRAPH\n",
    "    # bigram, trigrams, grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb9ff19-775b-456e-9b64-ad7db4c69a67",
   "metadata": {},
   "source": [
    "## Abstracts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5368f51-d8e2-4bc9-8e93-43ca621f6daa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rval_abstracts = word_count(clean_data['Abstract'])\n",
    "barchart_top10(rval_abstracts, 'Abstracts')\n",
    "# uncomment to get a look\n",
    "#rval_abstract[0][0:10]  # top 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee4218d-daea-4288-a81d-3ceca93029f3",
   "metadata": {},
   "source": [
    "## Analysis in the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c18b8c-31d9-4ad3-bb1c-93bf2fcf5db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rval_titles = word_count(clean_data['Article Title'])\n",
    "barchart_top10(rval_titles, 'Article Title')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ed7d4-dc10-4fb2-bcef-bd08a2bf4446",
   "metadata": {},
   "source": [
    "## Analysis in the Author's keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96a587b-5ca5-4867-9586-e4da68c17c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rval_keywords = word_count(clean_data['Author Keywords'])\n",
    "barchart_top10(rval_keywords, 'Author Keywords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1afbe41-e14f-4b1d-a8d6-0bcfe300eebe",
   "metadata": {},
   "source": [
    "# Data save and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d30c36-b40d-4064-b2f3-4bbe4a222403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_data(rval, datatitle):\n",
    "\n",
    "    # input: rval is the data extracted, datatitle is a string to identify the data saved\n",
    "    # output: export data analysis to csv.\n",
    "    \n",
    "    # If there is no folder for the result create one\n",
    "    os.makedirs('Results/' + file_export_ID, exist_ok=True)\n",
    "\n",
    "    # Word count data\n",
    "    rval[0].to_csv('./Results/' + file_export_ID + '/ILRA_WordsOccurence_' + file_export_ID +'_'+ datatitle + '.csv', sep=',')\n",
    "\n",
    "    # bigrams\n",
    "    bigrams_df = pd.DataFrame(list(rval[1]))\n",
    "    bigrams_df.to_csv('./Results/' + file_export_ID + '/ILRA_bigrams_' + file_export_ID +'_'+ datatitle + '.csv', sep=',')\n",
    "    #trigrams\n",
    "    trigrams_df = pd.DataFrame(list(rval[2]))\n",
    "    trigrams_df.to_csv('./Results/' + file_export_ID + '/ILRA_trigrams_' + file_export_ID +'_'+ datatitle + '.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6664f6-a432-4d00-b93c-903bafd2a010",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_data(rval_abstracts, 'abstract')\n",
    "save_data(rval_titles, 'title')\n",
    "save_data(rval_keywords, 'keywords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b053ed4a-24d2-4b7b-973b-e9f4f5c295ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rval_abstracts[1][0:10]  # sanity check of the top ten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548dfd0-016b-426c-accd-8f8cc85c7f5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
